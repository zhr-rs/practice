import jieba
from collections import Counter
import re
from pyecharts import options as opts
from pyecharts.charts import WordCloud, Bar, Line, Pie
import os
import requests
from bs4 import BeautifulSoup
import glob
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from streamlit_echarts import st_pyecharts

# ====================== å…¨å±€é…ç½® ======================
# é…ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

# å…¨å±€User-Agentï¼ˆé¿å…é‡å¤å®šä¹‰ï¼‰
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# åŠ è½½å¤–éƒ¨åœç”¨è¯è¡¨ï¼ˆæ›¿ä»£ç¡¬ç¼–ç ï¼‰
def load_stopwords():
    """åŠ è½½åœç”¨è¯è¡¨"""
    stopwords = set()
    try:
        with open("stopwords.txt", "r", encoding="utf-8") as f:
            for line in f:
                word = line.strip()
                if word:
                    stopwords.add(word)
    except FileNotFoundError:
        st.warning("âš ï¸ æœªæ‰¾åˆ°stopwords.txtï¼Œä½¿ç”¨é»˜è®¤ç²¾ç®€åœç”¨è¯è¡¨")
        stopwords = {"çš„", "äº†", "æ˜¯", "è¿™", "é‚£", "åœ¨", "å’Œ", "å°±", "éƒ½", "ä¹Ÿ", "è¿˜"}
    return stopwords

STOP_WORDS = load_stopwords()

# ====================== ç½‘é¡µçˆ¬å–å‡½æ•° ======================
def get_webpage_content(url):
    """çˆ¬å–æŒ‡å®šURLçš„ç½‘é¡µæ­£æ–‡ï¼ˆé€‚é…fulong_news_contentå®¹å™¨ï¼‰"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding  # è‡ªåŠ¨è¯†åˆ«ç¼–ç 
        response.raise_for_status()  # æŠ›å‡ºHTTPé”™è¯¯

        soup = BeautifulSoup(response.text, "html.parser")
        # ç§»é™¤æ— å…³æ ‡ç­¾
        for tag in soup(["script", "style", "nav", "header", "footer", "aside", "iframe"]):
            tag.decompose()

        # æå–æ ¸å¿ƒæ­£æ–‡ï¼ˆå¯æ ¹æ®ç›®æ ‡ç½‘ç«™ä¿®æ”¹classåï¼‰
        content_div = soup.find("div", class_="fulong_news_content")
        if content_div:
            final_content = content_div.get_text(strip=True, separator="\n")
            # è¿‡æ»¤ç©ºè¡Œ
            final_content = "\n".join([line for line in final_content.split("\n") if line.strip()])
            return final_content
        else:
            # è°ƒè¯•ï¼šè¿”å›ç½‘é¡µå‰10ä¸ªdivçš„ç±»åï¼Œæ–¹ä¾¿é€‚é…å…¶ä»–ç½‘ç«™
            all_div_classes = [div.get("class") for div in soup.find_all("div", class_=True)[:10]]
            return f"âŒ æœªæ‰¾åˆ°'fulong_news_content'å®¹å™¨ï¼\nç½‘é¡µå‰10ä¸ªdivç±»åï¼š{all_div_classes}"

    except requests.exceptions.RequestException as e:
        return f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}"
    except Exception as e:
        return f"âŒ è§£æå¤±è´¥ï¼š{str(e)}"

# ====================== æ–‡ä»¶ä¿å­˜å‡½æ•° ======================
def save_content_to_file(content, file_name):
    """ä¿å­˜çˆ¬å–å†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆé¿å…è·¯å¾„é—®é¢˜ï¼‰
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, file_name)
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        st.success(f"âœ… {file_name} ä¿å­˜æˆåŠŸï¼è·¯å¾„ï¼š{file_path}")
    except Exception as e:
        st.error(f"âŒ {file_name} ä¿å­˜å¤±è´¥ï¼š{str(e)}")

# ====================== è¯é¢‘ç»Ÿè®¡+è¿‡æ»¤å‡½æ•° ======================
def get_single_file_word_freq(file_name, min_freq):
    """
    è¯»å–å•ä¸ªæ–°é—»æ–‡ä»¶ï¼Œåˆ†è¯+è¿‡æ»¤åœç”¨è¯/ä½é¢‘è¯ï¼Œè¿”å›å‰20è¯é¢‘
    :param file_name: å•ä¸ªæ–°é—»æ–‡ä»¶è·¯å¾„
    :param min_freq: æœ€å°è¯é¢‘é˜ˆå€¼
    :return: æ’åºåçš„å‰20è¯é¢‘å­—å…¸
    """
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            total_text = f.read()
    except Exception as e:
        st.warning(f"âš ï¸ è¯»å–{file_name}å¤±è´¥ï¼š{str(e)}")
        return {}
    
    if not total_text:
        return {}
    
    # æ–‡æœ¬æ¸…æ´—ï¼šåªä¿ç•™ä¸­æ–‡
    clean_text = re.sub(r"[^\u4e00-\u9fa5]", "", total_text)
    # åˆ†è¯ + è¿‡æ»¤åœç”¨è¯/å•å­—
    words = [w for w in jieba.lcut(clean_text) if w not in STOP_WORDS and len(w) > 1]
    # ç»Ÿè®¡è¯é¢‘ + è¿‡æ»¤ä½é¢‘è¯
    word_count = Counter(words)
    filtered_words = {word: freq for word, freq in word_count.items() if freq >= min_freq}
    # å–å‰20å¹¶æŒ‰è¯é¢‘é™åºæ’åº
    top20_words = dict(sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:20])
    return top20_words

def get_merged_file_word_freq(file_list, min_freq):
    """
    ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰åˆå¹¶æ‰€æœ‰æ–°é—»æ–‡ä»¶ï¼Œåˆ†è¯+è¿‡æ»¤åœç”¨è¯/ä½é¢‘è¯ï¼Œè¿”å›å‰20è¯é¢‘
    :param file_list: æ–°é—»æ–‡ä»¶åˆ—è¡¨
    :param min_freq: æœ€å°è¯é¢‘é˜ˆå€¼
    :return: æ’åºåçš„å‰20è¯é¢‘å­—å…¸
    """
    total_text = ""
    # åˆå¹¶æ‰€æœ‰æ–°é—»æ–‡ä»¶å†…å®¹
    for file_name in file_list:
        try:
            with open(file_name, "r", encoding="utf-8") as f:
                total_text += f.read() + "\n"
        except Exception as e:
            st.warning(f"âš ï¸ è¯»å–{file_name}å¤±è´¥ï¼š{str(e)}")
            continue
    
    if not total_text:
        return {}
    
    # æ–‡æœ¬æ¸…æ´—ï¼šåªä¿ç•™ä¸­æ–‡
    clean_text = re.sub(r"[^\u4e00-\u9fa5]", "", total_text)
    # åˆ†è¯ + è¿‡æ»¤åœç”¨è¯/å•å­—
    words = [w for w in jieba.lcut(clean_text) if w not in STOP_WORDS and len(w) > 1]
    # ç»Ÿè®¡è¯é¢‘ + è¿‡æ»¤ä½é¢‘è¯
    word_count = Counter(words)
    filtered_words = {word: freq for word, freq in word_count.items() if freq >= min_freq}
    # å–å‰20å¹¶æŒ‰è¯é¢‘é™åºæ’åº
    top20_words = dict(sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:20])
    return top20_words

# ====================== å¤šå›¾è¡¨æ¸²æŸ“å‡½æ•° ======================
def render_chart(chart_type, top20_words, title_suffix=""):
    """
    æ ¹æ®é€‰æ‹©çš„å›¾è¡¨ç±»å‹æ¸²æŸ“å¯è§†åŒ–å›¾å½¢
    :param chart_type: å›¾è¡¨ç±»å‹
    :param top20_words: è¯é¢‘å­—å…¸
    :param title_suffix: æ ‡é¢˜åç¼€ï¼ˆåŒºåˆ†ä¸åŒé“¾æ¥ï¼‰
    """
    words = list(top20_words.keys())
    freqs = list(top20_words.values())
    
    if not words:
        st.warning(f"âš ï¸ è¿‡æ»¤åæ— æœ‰æ•ˆè¯æ±‡ï¼{title_suffix} è¯·é™ä½'æœ€å°è¯é¢‘'é˜ˆå€¼")
        return
    
    # 1. æŸ±çŠ¶å›¾ï¼ˆè¯é¢‘å‰20ï¼‰
    if chart_type == "æŸ±çŠ¶å›¾ï¼ˆè¯é¢‘å‰20ï¼‰":
        bar = (
            Bar(init_opts=opts.InitOpts(width="1000px", height="600px"))
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", freqs, itemstyle_opts=opts.ItemStyleOpts(color="#1890ff"))
            .set_global_opts(
                title_opts=opts.TitleOpts(title=f"æ–°é—»æ–‡æœ¬è¯é¢‘å‰20 - æŸ±çŠ¶å›¾ {title_suffix}", title_textstyle_opts=opts.TextStyleOpts(font_size=16)),
                xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=-45)),
                yaxis_opts=opts.AxisOpts(name="å‡ºç°æ¬¡æ•°"),
                tooltip_opts=opts.TooltipOpts(trigger="axis", axis_pointer_type="shadow")
            )
        )
        st_pyecharts(bar)
    
    # 2. æ¨ªå‘æŸ±çŠ¶å›¾ï¼ˆè¯é¢‘å‰20ï¼‰
    elif chart_type == "æ¨ªå‘æŸ±çŠ¶å›¾ï¼ˆè¯é¢‘å‰20ï¼‰":
        bar = (
            Bar(init_opts=opts.InitOpts(width="1000px", height="600px"))
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", freqs, itemstyle_opts=opts.ItemStyleOpts(color="#52c41a"))
            .reversal_axis()  # åè½¬è½´ï¼Œè½¬ä¸ºæ¨ªå‘
            .set_global_opts(
                title_opts=opts.TitleOpts(title=f"æ–°é—»æ–‡æœ¬è¯é¢‘å‰20 - æ¨ªå‘æŸ±çŠ¶å›¾ {title_suffix}", title_textstyle_opts=opts.TextStyleOpts(font_size=16)),
                yaxis_opts=opts.AxisOpts(name="è¯æ±‡"),
                xaxis_opts=opts.AxisOpts(name="å‡ºç°æ¬¡æ•°"),
                tooltip_opts=opts.TooltipOpts(trigger="axis", axis_pointer_type="shadow")
            )
        )
        st_pyecharts(bar)
    
    # 3. æŠ˜çº¿å›¾ï¼ˆè¯é¢‘è¶‹åŠ¿ï¼‰
    elif chart_type == "æŠ˜çº¿å›¾ï¼ˆè¯é¢‘è¶‹åŠ¿ï¼‰":
        line = (
            Line(init_opts=opts.InitOpts(width="1000px", height="600px"))
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", freqs, is_smooth=True, itemstyle_opts=opts.ItemStyleOpts(color="#f5222d"))
            .set_global_opts(
                title_opts=opts.TitleOpts(title=f"æ–°é—»æ–‡æœ¬è¯é¢‘å‰20 - æŠ˜çº¿å›¾ {title_suffix}", title_textstyle_opts=opts.TextStyleOpts(font_size=16)),
                xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=-45)),
                yaxis_opts=opts.AxisOpts(name="å‡ºç°æ¬¡æ•°"),
                tooltip_opts=opts.TooltipOpts(trigger="axis")
            )
        )
        st_pyecharts(line)
    
    # 4. é¥¼å›¾ï¼ˆè¯é¢‘å æ¯”ï¼Œå–å‰10é¿å…é‡å ï¼‰
    elif chart_type == "é¥¼å›¾ï¼ˆè¯é¢‘å æ¯”ï¼‰":
        pie = (
            Pie(init_opts=opts.InitOpts(width="1000px", height="600px"))
            .add(
                series_name="è¯é¢‘å æ¯”",
                data_pair=list(zip(words[:10], freqs[:10])),
                radius=["30%", "75%"],
                center=["50%", "50%"]
            )
            .set_global_opts(
                title_opts=opts.TitleOpts(title=f"æ–°é—»æ–‡æœ¬è¯é¢‘å‰10 - é¥¼å›¾ {title_suffix}", title_textstyle_opts=opts.TextStyleOpts(font_size=16)),
                legend_opts=opts.LegendOpts(orient="vertical", pos_top="15%", pos_left="2%")
            )
            .set_series_opts(
                label_opts=opts.LabelOpts(formatter="{b}: {c}æ¬¡ ({d}%)")
            )
        )
        st_pyecharts(pie)
    
    # 5. é¢ç§¯å›¾ï¼ˆè¯é¢‘ç´¯ç§¯ï¼Œmatplotlibï¼‰
    elif chart_type == "é¢ç§¯å›¾ï¼ˆè¯é¢‘ç´¯ç§¯ï¼‰":
        fig, ax = plt.subplots(figsize=(12, 6))
        # ç»˜åˆ¶é¢ç§¯å›¾
        ax.fill_between(words, freqs, color="#fa8c16", alpha=0.5, label="è¯é¢‘")
        # å åŠ æŠ˜çº¿
        ax.plot(words, freqs, color="#fa8c16", linewidth=2)
        # é…ç½®æ ·å¼
        ax.set_title(f"æ–°é—»æ–‡æœ¬è¯é¢‘å‰20 - é¢ç§¯å›¾ {title_suffix}", fontsize=14, pad=20)
        ax.set_xlabel("è¯æ±‡", fontsize=12)
        ax.set_ylabel("å‡ºç°æ¬¡æ•°", fontsize=12)
        ax.tick_params(axis="x", rotation=45)
        ax.grid(axis="y", alpha=0.3)
        ax.legend()
        plt.tight_layout()  # é€‚é…å¸ƒå±€
        st.pyplot(fig)
    
    # 6. çƒ­åŠ›å›¾ï¼ˆè¯é¢‘çŸ©é˜µï¼Œmatplotlibï¼‰
    elif chart_type == "çƒ­åŠ›å›¾ï¼ˆè¯é¢‘çŸ©é˜µï¼‰":
        # æ„é€ 4è¡Œ5åˆ—çŸ©é˜µï¼ˆå‰20è¯é¢‘ï¼‰
        if len(freqs) < 20:
            # ä¸è¶³20ä¸ªæ—¶è¡¥0
            freqs += [0] * (20 - len(freqs))
            words += [""] * (20 - len(words))
        heatmap_data = np.array(freqs).reshape(4, 5)
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        im = ax.imshow(heatmap_data, cmap="YlOrRd")
        # è®¾ç½®åæ ‡è½´æ ‡ç­¾
        ax.set_xticks(range(5))
        ax.set_yticks(range(4))
        ax.set_xticklabels(words[:5], rotation=45)
        ax.set_yticklabels([f"ç¬¬{i*5+1}-{i*5+5}å" for i in range(4)])
        # æ ‡æ³¨æ•°å€¼
        for i in range(4):
            for j in range(5):
                text = ax.text(j, i, heatmap_data[i, j], ha="center", va="center", color="black", fontsize=10)
        # é…ç½®æ ·å¼
        plt.colorbar(im, ax=ax, label="è¯é¢‘")
        ax.set_title(f"æ–°é—»æ–‡æœ¬è¯é¢‘å‰20 - çƒ­åŠ›å›¾ {title_suffix}", fontsize=14, pad=20)
        plt.tight_layout()
        st.pyplot(fig)
    
    # 7. è¯äº‘å›¾ï¼ˆpyechartsï¼‰
    elif chart_type == "è¯äº‘å›¾ï¼ˆé‡ç‚¹è¯å¯è§†åŒ–ï¼‰":
        word_cloud = (
            WordCloud(init_opts=opts.InitOpts(width="1000px", height="600px"))
            .add(
                series_name="è¯é¢‘",
                data_pair=list(top20_words.items()),
                word_size_range=[15, 100],
                shape="circle"  # è¯äº‘å½¢çŠ¶ï¼šcircle/rect/triangleç­‰
            )
            .set_global_opts(
                title_opts=opts.TitleOpts(title=f"æ–°é—»æ–‡æœ¬è¯é¢‘ - è¯äº‘å›¾ {title_suffix}", title_textstyle_opts=opts.TextStyleOpts(font_size=16)),
                legend_opts=opts.LegendOpts(is_show=False)
            )
        )
        st_pyecharts(word_cloud)
    
    # 8. ç¾åŒ–è¡¨æ ¼ï¼ˆæ•°æ®å±•ç¤ºï¼‰
    elif chart_type == "è¡¨æ ¼ï¼ˆè¯é¢‘å‰20æ•°æ®ï¼‰":
        df = pd.DataFrame({
            "æ’å": range(1, len(words)+1),
            "è¯æ±‡": words,
            "è¯é¢‘": freqs
        })
        # è¡¨æ ¼ç¾åŒ–ï¼ˆè¯é¢‘åˆ—æ¸å˜ç€è‰²ï¼‰
        styled_df = df.style.background_gradient(cmap="YlOrRd", subset=["è¯é¢‘"]) \
                          .set_properties(**{"text-align": "center"}) \
                          .set_table_styles([{"selector": "th", "props": [("font-size", "12px")]}])
        st.dataframe(styled_df, use_container_width=True)

# ====================== Streamlitä¸»äº¤äº’é€»è¾‘ ======================
if __name__ == "__main__":
    # é¡µé¢åŸºç¡€é…ç½®
    st.set_page_config(
        page_title="æ–°é—»æ–‡æœ¬è¯é¢‘åˆ†æç³»ç»Ÿ",
        page_icon="ğŸ“ˆ",
        layout="wide"  # å®½å±å¸ƒå±€
    )

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ“ˆ æ–°é—»æ–‡æœ¬è¯é¢‘åˆ†æç³»ç»Ÿ")
    st.divider()  # åˆ†éš”çº¿

    # ---------------------- ä¾§è¾¹æ äº¤äº’åŒº ----------------------
    with st.sidebar:
        st.title("ğŸ”§ äº¤äº’é…ç½®")
        st.divider()
        # 1. è¾“å…¥æ–‡ç« URLï¼ˆå¤šé“¾æ¥ç”¨è‹±æ–‡é€—å·åˆ†å‰²ï¼‰
        url_input = st.text_area(
            label="ğŸ“ æ–‡ç« é“¾æ¥",
            placeholder="ç¤ºä¾‹ï¼šhttps://xxx.com/1.html,https://xxx.com/2.html",
            height=100
        )
        # 2. ä½é¢‘è¯è¿‡æ»¤æ»‘å—
        min_freq = st.slider(
            label="ğŸ§¹ æœ€å°è¯é¢‘é˜ˆå€¼",
            min_value=1,
            max_value=20,
            value=2,
            step=1,
            help="è¿‡æ»¤å‡ºç°æ¬¡æ•°å°‘äºè¯¥å€¼çš„è¯æ±‡"
        )
        # æ–°å¢ï¼šåˆ†ææ¨¡å¼é€‰æ‹©
        analysis_mode = st.radio(
            label="ğŸ” åˆ†ææ¨¡å¼",
            options=["å•ç‹¬åˆ†ææ¯ä¸ªé“¾æ¥", "åˆå¹¶æ‰€æœ‰é“¾æ¥åˆ†æ"],
            index=0,
            help="é€‰æ‹©ã€Œå•ç‹¬åˆ†æã€å°†ä¸ºæ¯ä¸ªé“¾æ¥è¾“å‡ºç‹¬ç«‹ç»“æœï¼›ã€Œåˆå¹¶åˆ†æã€è¾“å‡ºç»¼åˆç»“æœï¼ˆåŸæœ‰é€»è¾‘ï¼‰"
        )
        # 3. å›¾è¡¨ç±»å‹é€‰æ‹©
        chart_type = st.selectbox(
            label="ğŸ“Š å¯è§†åŒ–å›¾è¡¨",
            options=[
                "æŸ±çŠ¶å›¾ï¼ˆè¯é¢‘å‰20ï¼‰",
                "æ¨ªå‘æŸ±çŠ¶å›¾ï¼ˆè¯é¢‘å‰20ï¼‰",
                "æŠ˜çº¿å›¾ï¼ˆè¯é¢‘è¶‹åŠ¿ï¼‰",
                "é¥¼å›¾ï¼ˆè¯é¢‘å æ¯”ï¼‰",
                "é¢ç§¯å›¾ï¼ˆè¯é¢‘ç´¯ç§¯ï¼‰",
                "çƒ­åŠ›å›¾ï¼ˆè¯é¢‘çŸ©é˜µï¼‰",
                "è¯äº‘å›¾ï¼ˆé‡ç‚¹è¯å¯è§†åŒ–ï¼‰",
                "è¡¨æ ¼ï¼ˆè¯é¢‘å‰20æ•°æ®ï¼‰"
            ],
            index=0
        )
        st.divider()
        # 4. æ‰§è¡ŒæŒ‰é’®
        run_analysis = st.button("ğŸš€ å¼€å§‹çˆ¬å–å¹¶åˆ†æ", type="primary")

    # ---------------------- æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ ----------------------
    if run_analysis:
        if not url_input:
            st.error("âŒ è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæ–‡ç« é“¾æ¥ï¼")
        else:
            # åˆ†å‰²URLå¹¶å»é‡/å»ç©ºæ ¼
            ARTICLE_URLS = [url.strip() for url in url_input.split(",") if url.strip()]
            if not ARTICLE_URLS:
                st.error("âŒ é“¾æ¥æ ¼å¼é”™è¯¯ï¼è¯·ç”¨è‹±æ–‡é€—å·åˆ†å‰²å¤šä¸ªé“¾æ¥")
            else:
                # çˆ¬å–å¹¶ä¿å­˜æ¯ç¯‡æ–‡ç« 
                st.subheader("ğŸ” çˆ¬å–è¿›åº¦")
                file_list = []  # å­˜å‚¨çˆ¬å–æˆåŠŸçš„æ–‡ä»¶è·¯å¾„
                for idx, url in enumerate(ARTICLE_URLS, start=1):
                    with st.expander(f"ç¬¬{idx}ç¯‡ï¼š{url}", expanded=False):
                        st.info(f"æ­£åœ¨çˆ¬å–...")
                        content = get_webpage_content(url)
                        file_name = f"news{idx}.txt"
                        save_content_to_file(content, file_name)
                        file_list.append(file_name)  # åŠ å…¥æ–‡ä»¶åˆ—è¡¨

                if not file_list:
                    st.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ–°é—»æ–‡ä»¶ï¼ˆnews1.txt/news2.txtç­‰ï¼‰ï¼")
                else:
                    st.success(f"âœ… å…±æ‰¾åˆ°{len(file_list)}ä¸ªæ–°é—»æ–‡ä»¶ï¼Œå¼€å§‹è¯é¢‘åˆ†æ...")
                    st.divider()

                    # æ¨¡å¼1ï¼šå•ç‹¬åˆ†ææ¯ä¸ªé“¾æ¥
                    if analysis_mode == "å•ç‹¬åˆ†ææ¯ä¸ªé“¾æ¥":
                        for idx, file_name in enumerate(file_list, start=1):
                            st.subheader(f"ğŸ“‹ ç¬¬{idx}ä¸ªé“¾æ¥ - è¯é¢‘æ’åå‰20ï¼ˆæ–‡ä»¶ï¼š{file_name}ï¼‰")
                            # å•ä¸ªæ–‡ä»¶è¯é¢‘åˆ†æ
                            top20_words = get_single_file_word_freq(file_name, min_freq)
                            if top20_words:
                                df_top20 = pd.DataFrame({
                                    "æ’å": range(1, len(top20_words)+1),
                                    "è¯æ±‡": list(top20_words.keys()),
                                    "è¯é¢‘": list(top20_words.values())
                                })
                                st.dataframe(df_top20, use_container_width=True)
                            else:
                                st.warning(f"âš ï¸ ç¬¬{idx}ä¸ªé“¾æ¥æ— ç¬¦åˆæ¡ä»¶çš„è¯æ±‡ï¼ˆè¯·é™ä½æœ€å°è¯é¢‘é˜ˆå€¼ï¼‰")
                            
                            # å•ä¸ªæ–‡ä»¶å¯è§†åŒ–
                            st.subheader(f"ğŸ“Š ç¬¬{idx}ä¸ªé“¾æ¥ - {chart_type}")
                            render_chart(chart_type, top20_words, title_suffix=f"ï¼ˆç¬¬{idx}ä¸ªé“¾æ¥ï¼‰")
                            st.divider()  # åˆ†éš”ä¸åŒé“¾æ¥çš„ç»“æœ

                    # æ¨¡å¼2ï¼šåˆå¹¶æ‰€æœ‰é“¾æ¥åˆ†æï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
                    else:
                        st.subheader("ğŸ“‹ æ‰€æœ‰é“¾æ¥åˆå¹¶ - è¯é¢‘æ’åå‰20")
                        top20_words = get_merged_file_word_freq(file_list, min_freq)
                        if top20_words:
                            df_top20 = pd.DataFrame({
                                "æ’å": range(1, len(top20_words)+1),
                                "è¯æ±‡": list(top20_words.keys()),
                                "è¯é¢‘": list(top20_words.values())
                            })
                            st.dataframe(df_top20, use_container_width=True)
                        else:
                            st.warning("âš ï¸ æ— ç¬¦åˆæ¡ä»¶çš„è¯æ±‡ï¼ˆè¯·é™ä½æœ€å°è¯é¢‘é˜ˆå€¼ï¼‰")

                        # åˆå¹¶ç»“æœå¯è§†åŒ–
                        st.subheader(f"ğŸ“Š æ‰€æœ‰é“¾æ¥åˆå¹¶ - {chart_type}")
                        render_chart(chart_type, top20_words, title_suffix="ï¼ˆæ‰€æœ‰é“¾æ¥åˆå¹¶ï¼‰")

    # ---------------------- è¾…åŠ©è¯´æ˜ ----------------------
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        ### ä½¿ç”¨æ­¥éª¤ï¼š
        1. åœ¨ä¾§è¾¹æ è¾“å…¥æ–‡ç« é“¾æ¥ï¼ˆå¤šä¸ªé“¾æ¥ç”¨è‹±æ–‡é€—å·`,`åˆ†å‰²ï¼‰ï¼›
        2. è°ƒæ•´ã€Œæœ€å°è¯é¢‘é˜ˆå€¼ã€ï¼ˆè¿‡æ»¤ä½é¢‘æ— æ„ä¹‰è¯æ±‡ï¼‰ï¼›
        3. é€‰æ‹©ã€Œåˆ†ææ¨¡å¼ã€ï¼šå•ç‹¬åˆ†ææ¯ä¸ªé“¾æ¥ / åˆå¹¶æ‰€æœ‰é“¾æ¥åˆ†æï¼›
        4. é€‰æ‹©éœ€è¦å±•ç¤ºçš„å¯è§†åŒ–å›¾è¡¨ç±»å‹ï¼›
        5. ç‚¹å‡»ã€Œå¼€å§‹çˆ¬å–å¹¶åˆ†æã€æŒ‰é’®ï¼Œç­‰å¾…ç»“æœã€‚

        ### é€‚é…è¯´æ˜ï¼š
        - çˆ¬å–é€»è¾‘é»˜è®¤é€‚é…classä¸º`fulong_news_content`çš„ç½‘ç«™ï¼Œå¯ä¿®æ”¹`app.py`ä¸­`get_webpage_content`å‡½æ•°çš„classåé€‚é…å…¶ä»–ç½‘ç«™ï¼›
        - åœç”¨è¯è¡¨å¯åœ¨`stopwords.txt`ä¸­æ‰©å±•/ä¿®æ”¹ï¼›
        - ç”Ÿæˆçš„`news1.txt/news2.txt`ç­‰æ–‡ä»¶ä¼šä¿å­˜åœ¨è„šæœ¬åŒçº§ç›®å½•ã€‚
        """)
